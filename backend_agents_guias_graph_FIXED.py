# backend/agents/guias/graph.py
"""
Grafo LangGraph para gera√ß√£o de guias.
VERS√ÉO CORRIGIDA - Processamento paralelo funcional
"""

from langgraph.graph import StateGraph, END
from .state import GuiaState
from .nodes.gerador_node import gerador_node
from .nodes.revisor_node import revisor_node
from .nodes.salvar_node import salvar_node
import asyncio
from typing import List
from backend.utils.logger import logger


def create_guias_graph():
    """Cria grafo LangGraph para gera√ß√£o de guias."""
    
    workflow = StateGraph(GuiaState)
    
    # Adiciona nodes
    workflow.add_node("gerar", gerador_node)
    workflow.add_node("revisar", revisor_node)
    workflow.add_node("salvar", salvar_node)
    
    # Entry point
    workflow.set_entry_point("gerar")
    
    # Edges
    workflow.add_edge("gerar", "revisar")
    
    # Edge condicional (revisar ‚Üí gerar ou salvar)
    def should_retry(state: GuiaState) -> str:
        topico = next(t for t in state["topicos"] if t["id"] == state["topico_atual_id"])
        
        if topico["status"] == "gerando":
            return "gerar"  # Retry
        else:
            return "salvar"  # Aprovado
    
    workflow.add_conditional_edges(
        "revisar",
        should_retry,
        {"gerar": "gerar", "salvar": "salvar"}
    )
    
    workflow.add_edge("salvar", END)
    
    return workflow.compile()


# ============================================
# PROCESSAMENTO DE UM √öNICO T√ìPICO
# ============================================

async def processar_topico(
    state_base: GuiaState,
    topico_id: str,
    graph
) -> tuple[str, dict]:
    """
    Processa um √∫nico t√≥pico.
    
    Args:
        state_base: Estado base (compartilhado, mas n√£o modificado)
        topico_id: ID do t√≥pico a processar
        graph: Grafo compilado
        
    Returns:
        tuple: (topico_id, topico_atualizado)
    """
    
    # Cria c√≥pia do state para este t√≥pico
    # IMPORTANTE: Cada t√≥pico precisa de seu pr√≥prio state isolado
    state_topico = state_base.copy()
    state_topico["topico_atual_id"] = topico_id
    
    # Pega o t√≥pico espec√≠fico
    topico = next(t for t in state_topico["topicos"] if t["id"] == topico_id)
    
    logger.info(f"üéØ [Paralelo] Iniciando: {topico['nome_completo']}")
    
    try:
        # Executa grafo para este t√≥pico
        final_state = await graph.ainvoke(state_topico)
        
        # Pega o t√≥pico atualizado
        topico_atualizado = next(
            t for t in final_state["topicos"] 
            if t["id"] == topico_id
        )
        
        if topico_atualizado["status"] == "concluido":
            logger.success(f"‚úÖ [Paralelo] Conclu√≠do: {topico['nome_completo']}")
        else:
            logger.warning(f"‚ö†Ô∏è [Paralelo] Status: {topico_atualizado['status']} - {topico['nome_completo']}")
        
        return topico_id, topico_atualizado
        
    except Exception as e:
        logger.error(f"‚ùå [Paralelo] Erro em {topico['nome_completo']}: {e}")
        
        # Marca como erro
        topico["status"] = "erro_fatal"
        topico["erro"] = {
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }
        
        return topico_id, topico


# ============================================
# PROCESSAMENTO PARALELO DE M√öLTIPLOS T√ìPICOS
# ============================================

async def processar_topicos_paralelo(
    state: GuiaState,
    graph,
    max_paralelo: int = 3
) -> List[dict]:
    """
    Processa m√∫ltiplos t√≥picos em paralelo.
    
    Args:
        state: Estado base
        graph: Grafo compilado
        max_paralelo: M√°ximo de t√≥picos simult√¢neos
        
    Returns:
        Lista de t√≥picos processados
    """
    
    topicos_ids = [t["id"] for t in state["topicos"]]
    total_topicos = len(topicos_ids)
    
    logger.info(
        f"üöÄ Iniciando processamento PARALELO de {total_topicos} t√≥pico(s)\n"
        f"   ‚öôÔ∏è Max simult√¢neos: {max_paralelo}"
    )
    
    # Sem√°foro para limitar concorr√™ncia
    semaphore = asyncio.Semaphore(max_paralelo)
    
    async def process_with_semaphore(topico_id: str):
        """Wrapper que usa sem√°foro para limitar concorr√™ncia."""
        async with semaphore:
            return await processar_topico(state, topico_id, graph)
    
    # Cria tasks para todos os t√≥picos
    tasks = [process_with_semaphore(tid) for tid in topicos_ids]
    
    # Executa todas as tasks em paralelo (respeitando max_paralelo)
    logger.info(f"‚è≥ Aguardando conclus√£o de {total_topicos} t√≥pico(s)...")
    
    resultados = await asyncio.gather(
        *tasks,
        return_exceptions=True  # N√£o para se um falhar
    )
    
    # Processa resultados
    topicos_processados = {}
    
    for i, resultado in enumerate(resultados):
        if isinstance(resultado, Exception):
            logger.error(f"‚ùå T√≥pico {i+1} falhou com exce√ß√£o: {resultado}")
            # Usa t√≥pico original com status de erro
            topico_original = state["topicos"][i]
            topico_original["status"] = "erro_fatal"
            topico_original["erro"] = {
                "message": str(resultado),
                "timestamp": datetime.now().isoformat()
            }
            topicos_processados[topico_original["id"]] = topico_original
        else:
            topico_id, topico_atualizado = resultado
            topicos_processados[topico_id] = topico_atualizado
    
    # Reconstr√≥i lista na ordem original
    topicos_finais = [
        topicos_processados[t["id"]]
        for t in state["topicos"]
    ]
    
    # Estat√≠sticas
    concluidos = sum(1 for t in topicos_finais if t["status"] == "concluido")
    erros = sum(1 for t in topicos_finais if t["status"] == "erro_fatal")
    
    logger.success(
        f"üéâ Processamento paralelo conclu√≠do!\n"
        f"   ‚úÖ Sucesso: {concluidos}/{total_topicos}\n"
        f"   ‚ùå Erros: {erros}/{total_topicos}"
    )
    
    return topicos_finais


# ============================================
# FUN√á√ÉO PRINCIPAL - EXECUTE GRAPH GUIAS
# ============================================

async def execute_graph_guias(config: dict, modo: str = "sequencial"):
    """
    Executa gera√ß√£o de guias para todos os t√≥picos.
    
    VERS√ÉO CORRIGIDA - Suporta processamento paralelo real.
    
    Args:
        config: Dict com configura√ß√£o do YAML
        modo: "sequencial" ou "paralelo"
    """
    from .state import criar_topico_inicial, criar_estatisticas_iniciais
    from datetime import datetime
    
    logger.info(f"üìã Modo de processamento: {modo.upper()}")
    
    # Cria state inicial
    state: GuiaState = {
        "projeto_nome": config["projeto"]["nome"],
        "area_conhecimento": config["projeto"]["area_conhecimento"],
        "radical_arquivo": config["projeto"]["radical_arquivo"],
        "pasta_saida": config["projeto"].get("pasta_saida_guias", "output/guias"),
        
        "topicos": [
            criar_topico_inicial(i, nome, config["projeto"]["radical_arquivo"])
            for i, nome in enumerate(config["topicos"])
        ],
        
        "llm_gerador_provider": config["modelos_guias"]["gerador"]["provedor"],
        "llm_gerador_modelo": config["modelos_guias"]["gerador"]["modelo"],
        "llm_gerador_temperatura": config["modelos_guias"]["gerador"].get("temperatura", 0.7),
        "llm_gerador_max_tokens": config["modelos_guias"]["gerador"].get("max_tokens", 8000),
        
        "llm_revisor_provider": config["modelos_guias"]["revisor"]["provedor"],
        "llm_revisor_modelo": config["modelos_guias"]["revisor"]["modelo"],
        "llm_revisor_temperatura": config["modelos_guias"]["revisor"].get("temperatura", 0.3),
        "llm_revisor_max_tokens": config["modelos_guias"]["revisor"].get("max_tokens", 2048),
        
        "max_paralelo": config["processamento"].get("max_paralelo", 3),
        "max_tentativas_revisao": config["processamento"].get("max_tentativas_revisao", 3),
        "delay_retry": config["processamento"].get("delay_retry", 5),
        
        "prompt_gerador": "",  # Carregado dos prompts.py
        "prompt_revisor": "",
        
        "status_geral": "processando",
        "topico_atual_id": None,
        "estatisticas": criar_estatisticas_iniciais(len(config["topicos"])),
        "logs": [],
        "erro_msg": None
    }
    
    graph = create_guias_graph()
    
    # ============================================
    # ESCOLHE MODO DE PROCESSAMENTO
    # ============================================
    
    if modo.lower() == "paralelo":
        # ‚úÖ PROCESSAMENTO PARALELO
        logger.info("üöÄ Usando processamento PARALELO")
        
        max_paralelo = state["max_paralelo"]
        
        topicos_finais = await processar_topicos_paralelo(
            state=state,
            graph=graph,
            max_paralelo=max_paralelo
        )
        
        # Atualiza state com t√≥picos processados
        state["topicos"] = topicos_finais
        
    else:
        # ‚úÖ PROCESSAMENTO SEQUENCIAL (original)
        logger.info("üìù Usando processamento SEQUENCIAL")
        
        for topico in state["topicos"]:
            state["topico_atual_id"] = topico["id"]
            
            logger.info(f"üéØ Processando: {topico['nome_completo']}")
            
            # Executa grafo
            final_state = await graph.ainvoke(state)
            
            # Atualiza o t√≥pico no state principal
            topico_atualizado = next(
                t for t in final_state["topicos"]
                if t["id"] == topico["id"]
            )
            
            # Atualiza in-place
            for key in topico_atualizado:
                topico[key] = topico_atualizado[key]
            
            if topico["status"] == "concluido":
                logger.success(f"‚úÖ Conclu√≠do: {topico['nome_completo']}")
    
    # ============================================
    # FINALIZA√á√ÉO
    # ============================================
    
    # Lista arquivos gerados
    arquivos_gerados = [
        t["nome_arquivo"]
        for t in state["topicos"]
        if t["status"] == "concluido" and t.get("nome_arquivo")
    ]
    
    state["status_geral"] = "concluido"
    state["arquivos_gerados"] = arquivos_gerados
    
    # Recalcula estat√≠sticas
    from .state import atualizar_estatisticas
    state["estatisticas"] = atualizar_estatisticas(state)
    
    logger.success(
        f"üéâ Processamento conclu√≠do!\n"
        f"   üìö Arquivos gerados: {len(arquivos_gerados)}\n"
        f"   ‚è±Ô∏è Modo: {modo.upper()}"
    )
    
    return {
        "status": "concluido",
        "status_geral": "concluido",
        "arquivos_gerados": arquivos_gerados,
        "estatisticas": state.get("estatisticas", {}),
        "logs": state.get("logs", [])
    }